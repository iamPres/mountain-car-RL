{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimal\n",
    "from torch.autograd import Variable\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')  # Make environment\n",
    "decay = 0.99\n",
    "alpha = 0.01\n",
    "gamma = 0.9\n",
    "max_episodes = 100  # Max episode\n",
    "epsilon = 1  # Mutation rate\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(x=[], y=[])])\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.hidden = 200\n",
    "        self.l1 = nn.Linear(self.state_space, self.hidden, bias=False)\n",
    "        self.l2 = nn.Linear(self.hidden, self.action_space, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            self.l2,\n",
    "        )\n",
    "        return model(x)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optimal.SGD(policy.parameters(), lr=alpha)\n",
    "scheduler = optimal.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "#progress_bar = tqdm(range(max_episodes), bar_format=\"{l_bar}{bar}|{postfix}\")\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    #progress_bar.update(1)\n",
    "    #progress_bar.set_postfix(Epsilon=str(round(epsilon, 2)))\n",
    "\n",
    "    epsilon *= decay\n",
    "    total_reward = 0\n",
    "\n",
    "    if epsilon < 0.1:\n",
    "        epsilon = 0.1\n",
    "\n",
    "    if ep > 90:\n",
    "        epsilon = 0\n",
    "        alpha = 0\n",
    "        gamma = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        #  sample Q values from policy\n",
    "        Q = policy.forward(torch.from_numpy(state).type(torch.FloatTensor))  # \" Q(s, a) \"\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #  find optimal action according to policy (over axis -1)\n",
    "            _, max = torch.max(Q, -1)\n",
    "            action = max.item()\n",
    "\n",
    "        new_state, reward, terminal, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        Q_new = policy.forward(torch.from_numpy(new_state).type(torch.FloatTensor))  # \" find Q (s', a') \"\n",
    "\n",
    "        #  find optimal action Q value for next step\n",
    "        new_max, _ = torch.max(Q_new, -1)  # \" max(Q(s', a')) \"\n",
    "\n",
    "        Q_target = Q.clone()\n",
    "        Q_target = Variable(Q_target.data)\n",
    "\n",
    "        #  update target value function according to TD\n",
    "        Q_target[action] = reward + torch.mul(new_max.detach(), gamma)  # \" reward + gamma*(max(Q(s', a')) \"\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(Q, Q_target)  # \" reward + gamma*(max(Q(s', a')) - Q(s, a)) \"\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Update original policy according to Q_target ( supervised learning )\n",
    "        policy.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #  Q and Q_target should converge\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    i=0\n",
    "    scatter = fig.data[0]\n",
    "    \n",
    "    while i<10:\n",
    "        new_x = [3+i, 4+i]\n",
    "        new_y = [3+i, i-4]\n",
    "        with fig.batch_update():\n",
    "            scatter.x += tuple(new_x)\n",
    "            scatter.y += tuple(new_y)\n",
    "        i = i+1\n",
    "    if ep % 10 == 0:\n",
    "        fig\n",
    "torch.save(policy.state_dict(), 'trained-10000.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
